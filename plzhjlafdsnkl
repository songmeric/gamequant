#pragma once

#include <algorithm>
#include <atomic>
#include <cstdint>
#include <cstring>
#include <memory>
#include <stdexcept>
#ifndef NDEBUG
#include <thread>
#include <cassert>
#endif
#include <cstdlib>
#include <deque>

namespace pme {

// SPSC RING BUFFER
// IMPORTANT: This implementation is intended for SINGLE-PRODUCER / SINGLE-CONSUMER usage.
// Simultaneous writes from multiple producers or reads from multiple consumers are undefined behaviour.
// If the consumer cannot keep up, new data is **silently dropped** (\`write()\` returns 0 and bytes get
// accounted in bytes_dropped_). The caller is responsible for monitoring \`getBytesDropped()\` and
// taking remediation (e.g. resize buffer, throttle producer).
//
// Memory layout: the backing buffer is allocated with CACHE_LINE_SIZE (64-byte) alignment to guarantee
// that every cache line belongs exclusively to this buffer and to avoid false sharing.

class RingBuffer {
private:
    static constexpr size_t CACHE_LINE_SIZE = 64;
    static constexpr size_t DEFAULT_SIZE = 1024 * 1024;
    static constexpr size_t MAX_SIZE = 64 * 1024 * 1024;
    
    static size_t nextPowerOf2(size_t n) {
        if (n == 0) return 1;
        if (n > MAX_SIZE) return MAX_SIZE;

        n--;
        n |= n >> 1;
        n |= n >> 2;
        n |= n >> 4;
        n |= n >> 8;
        n |= n >> 16;
#if SIZE_MAX > UINT32_MAX
        n |= n >> 32;
#endif
        n++;
        return n;
    }

    // Helper RAII deleter for aligned allocations
    struct AlignedDeleter {
        void operator()(void* p) const noexcept {
            std::free(p);
        }
    };

    using aligned_unique_ptr = std::unique_ptr<uint8_t, AlignedDeleter>;

    struct StaleBuffer {
        aligned_unique_ptr ptr;
        size_t safe_after_idx; // reader index after which it can be freed
    };
    std::deque<StaleBuffer> stale_;

    alignas(CACHE_LINE_SIZE) std::atomic<size_t> write_pos_{0};
    alignas(CACHE_LINE_SIZE) std::atomic<size_t> read_pos_{0};
    alignas(CACHE_LINE_SIZE) std::atomic<uint64_t> bytes_written_{0};
    alignas(CACHE_LINE_SIZE) std::atomic<uint64_t> bytes_dropped_{0};
    
    aligned_unique_ptr owned_buffer_;
    uint8_t* buffer_;
    size_t capacity_;
    size_t mask_;
#ifndef NDEBUG
    // Debug-only thread ownership tracking to ensure SPSC discipline.
    std::thread::id producer_thread_{}, consumer_thread_{};
#endif

    // Attempt to grow to accommodate at least "extra" additional bytes beyond current capacity.
    // Returns true on success, false if MAX_SIZE would be exceeded or allocation fails.
    bool grow(size_t extra) {
        if (capacity_ == MAX_SIZE) return false;

        size_t new_cap = nextPowerOf2(std::min(capacity_ + extra + 1, MAX_SIZE));
        if (new_cap <= capacity_) return false; // could not grow (already at max)

        void* ptr = nullptr;
        if (posix_memalign(&ptr, CACHE_LINE_SIZE, new_cap) != 0) {
            return false;
        }
        uint8_t* new_buf = static_cast<uint8_t*>(ptr);

        // Copy existing readable data from old buffer to new buffer preserving indices
        const size_t read  = read_pos_.load(std::memory_order_acquire);
        const size_t write = write_pos_.load(std::memory_order_relaxed);
        const size_t readable = write - read;

        const size_t old_mask = mask_;
        size_t read_idx = read & old_mask;

        size_t first_chunk = std::min(readable, capacity_ - read_idx);
        std::memcpy(new_buf + read_idx, buffer_ + read_idx, first_chunk);
        if (first_chunk < readable) {
            std::memcpy(new_buf, buffer_, readable - first_chunk);
        }

        // Stash old buffer until consumer passes "write"
        stale_.push_back(StaleBuffer{std::move(owned_buffer_), write});

        // Swap in new buffer
        buffer_ = new_buf;
        capacity_ = new_cap;
        mask_ = new_cap - 1;
        owned_buffer_.reset(new_buf);

        return true;
    }

public:
    explicit RingBuffer(size_t capacity = DEFAULT_SIZE)
        : capacity_(nextPowerOf2(std::min(capacity, MAX_SIZE))), mask_(capacity_ - 1) {
        void* ptr = nullptr;
        if (posix_memalign(&ptr, CACHE_LINE_SIZE, capacity_) != 0) {
            throw std::bad_alloc();
        }
        buffer_ = static_cast<uint8_t*>(ptr);
        owned_buffer_.reset(buffer_);
    }
    
    // Construct a RingBuffer around an externally-owned memory region.
    // The caller MUST provide a power-of-two sized, cache-line aligned buffer.

    RingBuffer(uint8_t* external_buffer, size_t capacity)
        : buffer_(external_buffer), capacity_(capacity), mask_(capacity_ - 1) {
        const bool invalid_capacity = (capacity == 0) || (capacity > MAX_SIZE) || ((capacity & (capacity - 1)) != 0);
        if (!buffer_ || invalid_capacity) {
            throw std::invalid_argument("External buffer must be non-null, power-of-two sized and <= MAX_SIZE");
        }
        if ((reinterpret_cast<uintptr_t>(buffer_) % CACHE_LINE_SIZE) != 0) {
            throw std::invalid_argument("External buffer must be cache-line aligned");
        }
    }
    
    RingBuffer(const RingBuffer&) = delete;
    RingBuffer& operator=(const RingBuffer&) = delete;
    
    RingBuffer(RingBuffer&& other) noexcept
        : write_pos_(other.write_pos_.load()), read_pos_(other.read_pos_.load()),
          bytes_written_(other.bytes_written_.load()), bytes_dropped_(other.bytes_dropped_.load()),
          owned_buffer_(std::move(other.owned_buffer_)), buffer_(other.buffer_),
          capacity_(other.capacity_), mask_(other.mask_) {
        other.buffer_ = nullptr;
        other.capacity_ = 0;
        other.mask_ = 0;
        other.write_pos_.store(0, std::memory_order_relaxed);
        other.read_pos_.store(0, std::memory_order_relaxed);
        other.bytes_written_.store(0, std::memory_order_relaxed);
        other.bytes_dropped_.store(0, std::memory_order_relaxed);
    }
    
    // Write up to 'len' bytes. Returns the number of bytes actually written.
    // Thread-safe for a single producer thread.
    [[nodiscard]] size_t write(const uint8_t* data, size_t len) noexcept {
#ifndef NDEBUG
        // Capture producer thread id on first use and assert consistency afterwards.
        if (producer_thread_ == std::thread::id()) {
            producer_thread_ = std::this_thread::get_id();
        } else {
            assert(producer_thread_ == std::this_thread::get_id() && "RingBuffer::write called from multiple producer threads");
        }
#endif
        if (!data || len == 0) {
            return 0;
        }

        // Load the current indices. For SPSC we need to observe the latest
        // consumer progress before claiming free space, hence acquire when
        // reading the consumer owned read_pos_.
        const size_t write = write_pos_.load(std::memory_order_relaxed);
        const size_t read  = read_pos_.load(std::memory_order_acquire);

        // Reclaim stale buffers whose data have been fully consumed
        while (!stale_.empty() && stale_.front().safe_after_idx <= read) {
            stale_.pop_front();
        }

        size_t available = (read + capacity_ - write - 1) & mask_;
        if (len > available) {
            if (!grow(len - available)) {
                // Could not grow – drop and report failure
                bytes_dropped_.fetch_add(len, std::memory_order_relaxed);
                return 0;
            }
            // After grow(), recalculate available with new capacity
            available = (read + capacity_ - write - 1) & mask_;
            assert(len <= available);
        }

        const size_t to_write = len; // now we have enough space

        const size_t write_idx   = write & mask_;
        const size_t first_chunk = std::min(to_write, capacity_ - write_idx);
        std::memcpy(buffer_ + write_idx, data, first_chunk);

        if (first_chunk < to_write) {
            std::memcpy(buffer_, data + first_chunk, to_write - first_chunk);
        }

        // Publish the new write position – release so that the data is visible
        // to the consumer before the index update becomes visible.
        write_pos_.store(write + to_write, std::memory_order_release);
        bytes_written_.fetch_add(to_write, std::memory_order_relaxed);

        if (to_write < len) {
            bytes_dropped_.fetch_add(len - to_write, std::memory_order_relaxed);
        }

        return to_write;
    }
    
    // Advance the consumer read pointer by up to 'len' bytes.
    // Thread-safe for a single consumer thread.
    void consume(size_t len) noexcept {
#ifndef NDEBUG
        // Capture consumer thread id on first use and assert consistency afterwards.
        if (consumer_thread_ == std::thread::id()) {
            consumer_thread_ = std::this_thread::get_id();
        } else {
            assert(consumer_thread_ == std::this_thread::get_id() && "RingBuffer::consume called from multiple consumer threads");
        }
#endif
        const size_t read  = read_pos_.load(std::memory_order_relaxed);
        const size_t write = write_pos_.load(std::memory_order_acquire);

        const size_t available  = write - read;
        const size_t to_consume = std::min(len, available);

        // Release so that prior reads by the consumer happen-before a producer
        // seeing the new read_pos_.
        read_pos_.store(read + to_consume, std::memory_order_release);
    }
    
    size_t available_data() const {
        size_t read  = read_pos_.load(std::memory_order_acquire);
        size_t write = write_pos_.load(std::memory_order_acquire);
        return write - read;
    }
    
    size_t available_space() const {
        const size_t read  = read_pos_.load(std::memory_order_acquire);
        const size_t write = write_pos_.load(std::memory_order_acquire);
        return (read + capacity_ - write - 1) & mask_;
    }
    
    // Reset the buffer – NOT thread-safe. Intended for initialisation / tests only.
    void reset() noexcept {
#ifndef NDEBUG
        producer_thread_ = std::thread::id();
        consumer_thread_ = std::thread::id();
#endif
        write_pos_.store(0, std::memory_order_release);
        read_pos_.store(0, std::memory_order_release);
        bytes_written_.store(0, std::memory_order_relaxed);
        bytes_dropped_.store(0, std::memory_order_relaxed);
    }
    
    uint64_t getBytesWritten() const { 
        return bytes_written_.load(std::memory_order_relaxed); 
    }
    
    uint64_t getBytesDropped() const { 
        return bytes_dropped_.load(std::memory_order_relaxed); 
    }
    
    size_t getCapacity() const { return capacity_; }
    
    bool empty() const noexcept {
        return read_pos_.load(std::memory_order_acquire) ==
               write_pos_.load(std::memory_order_acquire);
    }
    
    class Reader {
    private:
        const uint8_t* buffer_;
        size_t mask_;
        size_t start_pos_;
        size_t total_size_;
        
    public:
        Reader(const uint8_t* buffer, size_t mask, size_t start_pos, size_t total_size)
            : buffer_(buffer), mask_(mask), start_pos_(start_pos), total_size_(total_size) {}
        
        uint8_t operator[](size_t offset) const {
            if (offset >= total_size_) {
                throw std::out_of_range("Offset beyond available data");
            }
            return buffer_[(start_pos_ + offset) & mask_];
        }
        
        void read(void* dest, size_t offset, size_t len) const {
            if (offset + len > total_size_) {
                throw std::out_of_range("Read beyond available data");
            }
            
            uint8_t* dest_ptr = static_cast<uint8_t*>(dest);
            size_t pos = (start_pos_ + offset) & mask_;
            size_t buffer_size = mask_ + 1;
            
            size_t first_chunk = std::min(len, buffer_size - pos);
            std::memcpy(dest_ptr, buffer_ + pos, first_chunk);
            
            if (first_chunk < len) {
                std::memcpy(dest_ptr + first_chunk, buffer_, len - first_chunk);
            }
        }
        
        template<typename T>
        T read(size_t offset) const {
            T value;
            read(&value, offset, sizeof(T));
            return value;
        }
        
        size_t contiguous_from(size_t offset) const {
            if (offset >= total_size_) return 0;
            size_t pos = (start_pos_ + offset) & mask_;
            size_t buffer_size = mask_ + 1;
            return std::min(buffer_size - pos, total_size_ - offset);
        }
        
        size_t size() const { return total_size_; }
        
        const uint8_t* ptr_if_contiguous(size_t offset, size_t len) const {
            // Quick bounds check first.
            if (offset + len > total_size_) {
                return nullptr;
            }

            // Leverage contiguous_from() to determine how many bytes can be
            // read in a single, linear span starting at the given offset.
            // This avoids subtle off-by-one errors when the requested span
            // straddles the physical end of the circular buffer.
            if (contiguous_from(offset) < len) {
                return nullptr; // would wrap
            }

            size_t pos = (start_pos_ + offset) & mask_;
            return buffer_ + pos;
        }
    };
    
    Reader getReader() const noexcept {
        const size_t read  = read_pos_.load(std::memory_order_acquire);
        const size_t write = write_pos_.load(std::memory_order_acquire);
        const size_t available = write - read;
        const size_t read_idx  = read & mask_;
        return Reader(buffer_, mask_, read_idx, available);
    }

    // Compile-time checks for platform assumptions
    static_assert(sizeof(size_t) == sizeof(uintptr_t), "RingBuffer assumes size_t matches pointer width");
    static_assert((CACHE_LINE_SIZE & (CACHE_LINE_SIZE - 1)) == 0, "CACHE_LINE_SIZE must be power of two");
    static_assert(DEFAULT_SIZE <= MAX_SIZE, "DEFAULT_SIZE must not exceed MAX_SIZE");
};

} 
